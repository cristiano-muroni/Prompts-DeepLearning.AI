{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd569d01",
   "metadata": {},
   "source": [
    "# 4 - Inferring\n",
    " Além de extrair rótulos, extrair nomes, compreender o sentimento de um texto, \n",
    " Se você deseja extrair um sentimento positivo ou negativo de um texto,\n",
    " no fluxo tradicional de aprendizado de máquina, você teria que coletar o conjunto de dados\n",
    " de rótulos, treinar um modelo, descobrir como implantar o modelo em algum lugar na nuvem e\n",
    " fazer inferências. Isso pode funcionar bem, mas é muito trabalho para passar por esse processo.\n",
    " \n",
    " Além disso, para cada tarefa, como sentimento versus extração de nomes, você precisa treinar\n",
    "  e implantar um modelo separado. Uma das coisas realmente boas sobre os grandes modelos de\n",
    "  linguagem é que, para muitas tarefas como essas, você pode apenas escrever um prompt e começar\n",
    "  a gerar resultados quase imediatamente. Isso proporciona uma velocidade tremenda no desenvolvimento de aplicativos. E você também pode usar apenas um modelo, uma API, para fazer muitas tarefas diferentes, em vez de precisar descobrir como treinar e implantar muitos modelos diferentes. Com isso, vamos ao código para ver como você pode aproveitar isso.\n",
    "  \n",
    "  Aqui está nosso código inicial usual. Vou executá-lo. O exemplo mais adequado que usarei é uma análise para uma lâmpada. Então, \"Precisava de uma boa lâmpada para o quarto e esta tinha espaço adicional\" e assim por diante. Então, vou escrever um prompt para classificar o sentimento disso. E se eu quiser que o sistema me diga, sabe, qual é o sentimento, posso simplesmente escrever: \"Qual é o sentimento da seguinte análise do produto\", com o delimitador usual e o texto da análise, e assim por diante. E vamos executar isso. E isso diz: \"O sentimento da análise do produto é positivo\", o que na verdade parece bem correto. Essa lâmpada não é perfeita, mas o cliente parece bastante satisfeito. Parece ser uma empresa ótima que se preocupa com os clientes e produtos. Acho que o sentimento positivo parece ser a resposta correta. Agora isso imprime toda a sentença: \"O sentimento da análise do produto é positivo\". Se você quisesse uma resposta mais concisa para facilitar o pós-processamento, eu posso pegar esse prompt e adicionar outra instrução para dar respostas com apenas uma palavra, seja positivo ou negativo.\n",
    "  \n",
    "  Então, ele apenas imprime \"positivo\" assim, o que facilita para um pedaço de texto pegar essa saída e processá-la e fazer algo com ela. Vamos ver outro prompt, ainda usando a análise da lâmpada. Aqui, eu tenho, é \"Identificar uma lista de emoções que o autor da análise a seguir está expressando. Inclua no máximo cinco itens nesta lista.\" Então, os grandes modelos de linguagem são muito bons em extrair coisas específicas de um texto. Neste caso, estamos expressando as emoções e isso pode ser útil para entender como seus clientes pensam sobre um determinado produto. Para muitas organizações de suporte ao cliente, é importante entender se um usuário específico está extremamente chateado. Então, você pode ter um problema de classificação diferente, como \"O autor da análise a seguir está expressando raiva?\". Porque se alguém está realmente com raiva, pode ser necessário prestar mais atenção em uma análise do cliente, ter o suporte ao cliente ou o sucesso do cliente para descobrir o que está acontecendo e resolver as coisas para o cliente. Nesse caso, o cliente não está com raiva. E observe que, com o aprendizado supervisionado, se eu quisesse construir todos esses classificadores, não teria como fazer isso com o aprendizado supervisionado em apenas alguns minutos, como você viu eu fazer neste vídeo. \n",
    "  \n",
    " Talvez pergunte se o cliente está expressando encanto ou pergunte se há alguma peça faltando e veja se você pode criar um prompt para fazer diferentes inferências sobre essa análise da lâmpada.\n",
    " \n",
    "  Vou mostrar mais algumas coisas que você pode fazer com este sistema, especificamente extrair\n",
    "  informações mais detalhadas de uma análise de cliente. A extração de informações é a parte do\n",
    "  PLN, do Processamento de Linguagem Natural, que se relaciona a pegar um pedaço de texto e\n",
    "  extrair certas coisas que você deseja saber do texto. Então, neste prompt, estou pedindo para\n",
    "  identificar os seguintes itens, o item comprado e o nome da empresa que fabricou o item.\n",
    "  \n",
    "  \n",
    "  Novamente, se você estiver tentando resumir muitas análises de um site de comércio eletrônico\n",
    "  de compras on-line, pode ser útil para sua grande coleção de análises descobrir quais foram os\n",
    "  itens, quem fabricou o item, descobrir o sentimento positivo e negativo, para acompanhar as\n",
    "  tendências sobre sentimento positivo ou negativo para itens específicos ou para fabricantes\n",
    "  específicos. E, neste exemplo, vou pedir a ele para formatar sua resposta como um objeto JSON\n",
    "  com \"Item\" e \"Marca\" como chaves. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "da820b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "openai.api_key = \"HERE YOUR API KEY\"\n",
    "\n",
    "def get_completion(prompt, model = \"gpt-3.5-turbo\"):\n",
    "    messages = [ {\"role\": \"user\", \"content\": prompt}]\n",
    "    response = openai.ChatCompletion.create(\n",
    "      model = model,\n",
    "      messages = messages,\n",
    "      temperature = 0,\n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b79b63f",
   "metadata": {},
   "source": [
    "#  Product review text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0896afef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "lamp_review = \"\"\"\n",
    "Precisava de uma boa lâmpada para o meu quarto, e esta tinha \\\n",
    "espaço adicional e um preço não muito alto. Chegou rapidamente.  \\\n",
    "O fio da lâmpada quebrou durante o transporte e a empresa prontamente \\\n",
    "enviou outro. Chegou em poucos dias também. Foi fácil de montar. \\\n",
    "Tive uma peça faltando, então entrei em contato com o suporte e eles \\\n",
    "rapidamente me enviaram a peça que faltava! Parece que a Lumina é uma \\\n",
    "ótima empresa que se preocupa com seus clientes e produtos!!\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d49553f",
   "metadata": {},
   "source": [
    "# Sentiment (positive/negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4b8f1773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O sentimento da análise é positivo.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Prompt para identificar o sentimento da análise do produto\n",
    "prompt = f\"\"\"\n",
    "Qual é o sentimento da seguinte análise do produto, \n",
    "que está delimitada com três acentos graves?\n",
    "\n",
    "Texto da análise: '''{lamp_review}'''\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea311f4",
   "metadata": {},
   "source": [
    "# Prompt para identificar o sentimento da análise do produto(positivo ou negativo)\n",
    "_fornecendo uma resposta mais concisa_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7cb9d7a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positivo\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prompt = f\"\"\"\n",
    "Qual é o sentimento da seguinte análise do produto, \n",
    "que está delimitada com três acentos graves?\n",
    "\n",
    "Dê sua resposta como uma única palavra, seja \"positivo\" \\\n",
    "ou \"negativo\".\n",
    "\n",
    "Texto da análise: '''{lamp_review}'''\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e90aab8",
   "metadata": {},
   "source": [
    "#  Identify types of emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "54426907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "satisfação, gratidão, confiança, surpresa, admiração\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#  Prompt para identificar uma lista de emoções expressas na análise do produto\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Identifique uma lista de emoções que o autor da \\\n",
    "seguinte análise está expressando. Inclua no máximo \\\n",
    "cinco itens na lista. Formate sua resposta como uma lista de \\\n",
    "palavras em minúsculas separadas por vírgulas.\n",
    "\n",
    "Texto da análise: '''{lamp_review}'''\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f153bc04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Não.\n"
     ]
    }
   ],
   "source": [
    "#  Identify anger\n",
    "\n",
    "#  Prompt para identificar se o autor da análise está expressando raiva\n",
    "\n",
    "prompt = f\"\"\"\n",
    "O autor da seguinte análise está expressando raiva? \\\n",
    "A análise está delimitada com três acentos graves. \\\n",
    "Dê sua resposta como \"sim\" ou \"não\".\n",
    "\n",
    "Texto da análise: '''{lamp_review}'''\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d187eb",
   "metadata": {},
   "source": [
    "# Identifique raiva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f6fcc843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Não\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "O autor da seguinte avaliação está expressando raiva?\\\n",
    "A avaliação está delimitada com três acentos graves. \\\n",
    "Forneça sua resposta como sim ou não.\n",
    "\n",
    "Texto da avaliação: '''{lamp_review}'''\n",
    "\"\"\"\n",
    "resposta = get_completion(prompt)\n",
    "print(resposta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c183170d",
   "metadata": {},
   "source": [
    "#  Extract product and company name from customer reviews\n",
    "\n",
    "_Prompt para identificar os seguintes itens do texto da análise:_\n",
    "   - Item comprado pelo revisor\n",
    "   - Empresa que fabricou o item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4a478a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"Item\": \"lâmpada\",\n",
      "  \"Marca\": \"Lumina\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "Identifique os seguintes itens do texto da análise:\n",
    "- Item comprado pelo revisor\n",
    "- Empresa que fabricou o item\n",
    "\n",
    "A análise está delimitada com três acentos graves.\n",
    "Formate sua resposta como um objeto JSON com as chaves \"Item\" e \"Marca\".\n",
    "Se a informação não estiver presente, use \"desconhecido\" como valor.\n",
    "Faça sua resposta o mais breve possível.\n",
    "\n",
    "Texto da análise: '''{lamp_review}'''\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfa27fc",
   "metadata": {},
   "source": [
    "#  Doing multiple tasks at once\n",
    "\n",
    "_Prompt para identificar os seguintes itens do texto da análise:_\n",
    " - Sentimento (positivo ou negativo)\n",
    " - O revisor está expressando raiva? (verdadeiro ou falso)\n",
    " - Item comprado pelo revisor\n",
    " - Empresa que fabricou o item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7a3139ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"Sentimento\": \"positivo\",\n",
      "  \"Raiva\": false,\n",
      "  \"Item\": \"lâmpada\",\n",
      "  \"Marca\": \"Lumina\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "Identifique os seguintes itens do texto da análise:\n",
    "- Sentimento (positivo ou negativo)\n",
    "- O revisor está expressando raiva? (verdadeiro ou falso)\n",
    "- Item comprado pelo revisor\n",
    "- Empresa que fabricou o item\n",
    "\n",
    "A análise está delimitada com três acentos graves.\n",
    "Formate sua resposta como um objeto JSON com as chaves \"Sentimento\", \"Raiva\", \"Item\" e \"Marca\".\n",
    "Se a informação não estiver presente, use \"desconhecido\" como valor.\n",
    "Faça sua resposta o mais breve possível.\n",
    "Formate o valor da Raiva como um booleano.\n",
    "\n",
    "Texto da análise: '''{lamp_review}'''\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b59512f",
   "metadata": {},
   "source": [
    "#  Inferring topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1649ecf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Pesquisa de satisfação\n",
      "2. Departamento mais popular\n",
      "3. NASA\n",
      "4. Administração da Previdência Social\n",
      "5. Melhoria da satisfação no trabalho\n"
     ]
    }
   ],
   "source": [
    "\n",
    "story = \"\"\"\n",
    "Em uma recente pesquisa conduzida pelo governo,\n",
    "funcionários do setor público foram solicitados a avaliar seu nível\n",
    "de satisfação com o departamento em que trabalham.\n",
    "Os resultados revelaram que a NASA era o departamento mais popular,\n",
    "com uma classificação de satisfação de 95%.\n",
    "\n",
    "Um funcionário da NASA, John Smith, comentou sobre os resultados,\n",
    "afirmando: \"Não estou surpreso que a NASA tenha ficado em primeiro lugar.\n",
    "É um ótimo lugar para trabalhar, com pessoas incríveis e oportunidades incríveis.\n",
    "Estou orgulhoso de fazer parte de uma organização tão inovadora.\"\n",
    "\n",
    "Os resultados também foram bem recebidos pela equipe de gerenciamento da NASA,\n",
    "com o diretor Tom Johnson declarando: \"Estamos encantados em saber que nossos funcionários estão satisfeitos com o trabalho na NASA.\n",
    "Temos uma equipe talentosa e dedicada que trabalha incansavelmente para alcançar nossos objetivos, e é fantástico ver que o trabalho árduo está valendo a pena.\"\n",
    "\n",
    "A pesquisa também revelou que a Administração da Previdência Social tinha a menor classificação de satisfação,\n",
    "com apenas 45% dos funcionários indicando que estavam satisfeitos com seu trabalho.\n",
    "O governo se comprometeu a abordar as preocupações levantadas pelos funcionários na pesquisa e trabalhar para melhorar a satisfação no trabalho em todos os departamentos.\n",
    "\"\"\"\n",
    "#  Inferring 5 topics\n",
    "prompt = f\"\"\"\n",
    "Determine cinco tópicos que estão sendo discutidos no texto a seguir, que está delimitado por três acentos graves.\n",
    "\n",
    "IMPORTANTE\n",
    "- Cada item deve ter somente uma ou duas palavras.\n",
    "- No caso de Orgão ou empresa traga somente o nome.\n",
    "\n",
    "Formate sua resposta como uma lista numerada de itens separados por vírgulas.\n",
    "\n",
    "Texto da amostra: '''{story}'''\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7880ef88",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.split(sep=',')\n",
    "\n",
    "topic_list = [\n",
    "    \"nasa\", \"governo local\", \"engenharia\", \n",
    "    \"satisfação do funcionário\", \"governo federal\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19550fa5",
   "metadata": {},
   "source": [
    "#  Make a news alert for certain topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2f58eb36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nasa: 1\n",
      "governo local: 0\n",
      "engenharia: 0\n",
      "satisfação do funcionário: 1\n",
      "governo federal: 1\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "Determine se cada item na seguinte lista de tópicos \\\n",
    "é um tópico no texto abaixo, que está delimitado por \\\n",
    "três acentos graves.\n",
    "\n",
    "Dê sua resposta como uma lista com o nome do topico separado por \":\" e com 0 ou 1 para cada tópico.\n",
    "\n",
    "Lista de tópicos: {\", \".join(topic_list)}\n",
    "\n",
    "Exemplo de texto: '''{story}'''\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3c371166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALERT: New NASA story!\n"
     ]
    }
   ],
   "source": [
    "topic_dict = {i.split(': ')[0]: int(i.split(': ')[1]) for i in response.split(sep='\\n')}\n",
    "if topic_dict['nasa'] == 1:\n",
    "    print(\"ALERT: New NASA story!\")\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70fcce1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
